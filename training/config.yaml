model_name: "meta-llama/Llama-3.2-8B-Instruct"
data_path: "data_pipeline/instruction_tuning.jsonl"
output_dir: "checkpoints/tax-llama-ind"
max_seq_length: 2048
load_in_4bit: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
learning_rate: 2.0e-4
num_train_epochs: 3
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
logging_steps: 10
save_steps: 100
optim: "paged_adamw_32bit"
